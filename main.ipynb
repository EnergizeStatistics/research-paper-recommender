{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Researcher Paper Recommender #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Tools ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "from fnmatch import fnmatch\n",
    "from pathlib import PurePath\n",
    "\n",
    "import urllib.request # download files \n",
    "import subprocess # operating system command\n",
    "\n",
    "########\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & unzip research paper dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-55033f4f9cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# download file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://www.comp.nus.edu.sg/%7Esugiyama/SchRecData/20100825-SchPaperRecData.zip'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworking_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'20100825-SchPaperRecData.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# unzip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# set dataset path\n",
    "working_dir = \"/home/lee/Documents/Datasets for GitHub/scholarly_paper_recommendation/\"\n",
    "\n",
    "# download file\n",
    "url = 'http://www.comp.nus.edu.sg/%7Esugiyama/SchRecData/20100825-SchPaperRecData.zip'  \n",
    "urllib.request.urlretrieve(url, working_dir+'20100825-SchPaperRecData.zip')\n",
    "\n",
    "# unzip file\n",
    "subprocess.run([\"unzip\", working_dir+\"20100825-SchPaperRecData.zip\", \"-d\", working_dir])  \n",
    "\n",
    "del url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the vectors that each represents a candidate paper to recommend ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_file_names_in_list(folder_name, pattern):\n",
    "    all_rec_name = []\n",
    "    for path, subdirs, files in os.walk(folder_name):\n",
    "        for name in files:\n",
    "            if fnmatch(name, pattern):\n",
    "                all_rec_name.append(PurePath(path, name))\n",
    "                \n",
    "    return all_rec_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the full path of all files in the paper directory\n",
    "all_paper_files = all_file_names_in_list(working_dir+'20100825-SchPaperRecData/RecCandidatePapersFV/', \"*_recfv.txt\")\n",
    "# list, convert PurePath object to string\n",
    "all_paper_files = [str(x) for x in all_paper_files]\n",
    "# list of tuples, [(paper id like \"P00-1001\", paper file full path), (paper id like \"P00-1002\", paper file full path)]\n",
    "all_paper_files = list(zip([x[117:125] for x in all_paper_files], all_paper_files))\n",
    "\n",
    "\"\"\"structure of all_df_original (which is a list):\n",
    "df[0] is a dataframe with 2 columns\n",
    "token tfidf_weight\n",
    "AAA 0.01\n",
    "\"\"\"\n",
    "all_df_original = [pd.read_csv(file, names=['token', 'tfidf_weight'], \\\n",
    "                 delim_whitespace=True, index_col=False, header=None) for file in list(zip(*all_paper_files))[1]]\n",
    "\n",
    "\"\"\" list\n",
    "dataframe with 1 column, token is column name\n",
    "AAA\n",
    "0.01\n",
    "\"\"\"\n",
    "all_rec_paper = [x.set_index('token').transpose().rename_axis('', axis=\"columns\").reset_index(drop=True)\\\n",
    "                     for x in all_df_original]\n",
    "\n",
    "del all_df_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all the vectors that each represents a researcher's interest ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here a researcher's past published papers represent his or her research interest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the full path of all files in the junior researcher directory\n",
    "all_researcher_files = all_file_names_in_list(working_dir+'20100825-SchPaperRecData/JuniorR/', \"*-?_fv.txt\")\n",
    "# list, convert PurePath object to string\n",
    "all_researcher_files = [str(x) for x in all_researcher_files]\n",
    "# list of tuples, the regex extracts the researcher id from path,\n",
    "# then [(researcher id like \"y1\", paper file full path), (researcher id like \"y2\", paper file full path)]\n",
    "all_researcher_files = list(zip([re.findall(re.escape(working_dir+\"20100825-SchPaperRecData/JuniorR/\")\\\n",
    "                                            +\"([^/]+)\"+re.escape(\"/\"), x)[0] for x in all_researcher_files],\\\n",
    "                                all_researcher_files))\n",
    "\n",
    "\"\"\"structure of all_df_original (which is a list):\n",
    "df[0] is a dataframe with 2 columns\n",
    "token tfidf_weight\n",
    "AAA 0.01\n",
    "\"\"\"\n",
    "all_researcher_original = [pd.read_csv(file, names=['token', 'tf_weight'], \\\n",
    "                 delim_whitespace=True, index_col=False, header=None) for file in list(zip(*all_researcher_files))[1]]\n",
    "\"\"\" list\n",
    "dataframe with 1 column, token is column name\n",
    "AAA\n",
    "0.01\n",
    "\"\"\"\n",
    "all_researcher_paper = [x.set_index('token').transpose().rename_axis('', axis=\"columns\").reset_index(drop=True)\\\n",
    "                     for x in all_researcher_original]\n",
    "\n",
    "del all_researcher_original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Recommendations ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def magnitude(v):\n",
    "#     return round(np.sqrt(sum([a * a for a in v])), 3)\n",
    "\n",
    "def cosine_similarity(x, y):\n",
    "    \"\"\"calculate cosine similarity based on formula from wikipedia\n",
    "    https://en.wikipedia.org/wiki/Cosine_similarity\n",
    "    \"\"\"\n",
    "#     numerator = sum(a * b for a, b in zip(x, y))\n",
    "#     denominator = square_rooted(x) * square_rooted(y)\n",
    "    numerator =  np.sqrt(x.dot(y))\n",
    "    denominator = np.linalg.norm(x) * np.linalg.norm(y)\n",
    "    \n",
    "    try:\n",
    "        cosine_similarity = round(numerator/float(denominator), 3)\n",
    "    except ZeroDivisionError:\n",
    "        cosine_similarity = 0\n",
    "\n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate similarity scores between a researcher's interest and all candidate papers ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'P01-1063': 1.329, 'P00-1079': 1.271, 'P00-1049': 1.331, 'P01-1062': 1.258, 'P04-1056': 1.19}\n"
     ]
    }
   ],
   "source": [
    "def n_most_similar_papers_selected_researcher(n_most, researcher):\n",
    "    rec_score = dict()\n",
    "    \n",
    "    # find where the selected researcher is in the list all_researcher_paper\n",
    "    researcher_index = list(zip(*all_researcher_files))[0].index(researcher)\n",
    "    \n",
    "    for i in range(len(all_rec_paper)):\n",
    "        together = pd.concat([all_rec_paper[i], all_researcher_paper[researcher_index]], axis=0, join='inner')    \n",
    "        rec_score[all_paper_files[i][0]] = cosine_similarity(together.iloc[0, :], together.iloc[1, :])\n",
    "        \n",
    "    # multiple keys have the same value    \n",
    "    return {key: value for key, value in rec_score.items() \\\n",
    "            if value in sorted(set(rec_score.values()), reverse=True)[:n_most]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick researcher \"y3\" and give 5 most similar papers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_most_similar_papers_selected_researcher(5, 'y3'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: \n",
    "The most similar paper may not be a good one to recommend as it might involve the targeted researcher already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"corpus\" is a set of all the words that ever appear in any papers\n",
    "# corpus = set(itertools.chain.from_iterable([list(a['token']) for a in df[0:10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"structure of all_tokens (a list):\n",
    "# all_tokens[0]\n",
    "# ['AAA', 'BBB']\n",
    "# \"\"\"\n",
    "# each_tokens = [list(one_paper_token['token'].astype('str')) for one_paper_token in all_df_original]\n",
    "\n",
    "# \"\"\"structure of all_docs (a list):\n",
    "# all_docs[0]\n",
    "# ['AAA' 'BBB']\n",
    "# \"\"\"\n",
    "# all_docs = [' '.join(x) for x in each_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"structore of all_tokens_matrix (a dataframe):\n",
    "# AAA BBB CCC (column names)\n",
    "# 0 1 0 (whether that word shows in paper #1)\n",
    "# 1 1 0 (whether that word shows in paper #2)\n",
    "# \"\"\"\n",
    "# vec = CountVectorizer()\n",
    "# corpus_matrix = vec.fit_transform(all_docs)\n",
    "# all_tokens_matrix = pd.DataFrame(corpus_matrix.toarray(), columns=vec.get_feature_names()).rename_axis('', axis='rows') # note this is a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
